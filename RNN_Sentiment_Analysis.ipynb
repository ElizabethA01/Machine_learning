{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhlf9JVyU5HW6NmbWe06Q6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizabethA01/Machine_learning/blob/main/RNN_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project**\n",
        "\n",
        "This project using natural language processing in the form of recurrent neural networks (RNN) to predict whether the movie reviews are positive or negative.\n",
        "\n",
        "It uses the imdb movie dataset. \n"
      ],
      "metadata": {
        "id": "qcpPhlebfjJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "uLwJG7Q8fXAj"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x \n",
        "from keras.datasets import imdb\n",
        "import keras.preprocessing\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at one review\n",
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsVSibJIhWAr",
        "outputId": "613f1dc1-dd3c-4ffb-b73d-2eb1c0223861"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ADDING PADDING IF LENGTHS ARE NOT THE SAME \n",
        "# you cannot pass different length data to the neural network so you need to padd the sequences\n",
        "\n",
        "# add padding if its less 250 words and trim if its more than 250 words\n",
        "\n",
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)\n",
        "                                   \n",
        "len(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MFOl3YKhtq-",
        "outputId": "40446679-0a84-457c-fd8b-a7f405507095"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the model**\n",
        "\n",
        "Use a word embedding layer as the first layer in the model and add a LSTM layer afterwards that feeds into a dense nod to get the predicted sentiment.\n",
        "\n",
        "32 stands for the ouput dimension of the vectors generated by the embedding "
      ],
      "metadata": {
        "id": "aFsnwto7ivlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "                             tf.keras.layers.LSTM(32),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid') # sigmoid is between 0 and 1. If more than 0.5 then considered a positive review.\n",
        "])"
      ],
      "metadata": {
        "id": "AxBiz_JiiRpE"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq2SRF0IjWhs",
        "outputId": "c9ce9933-280e-4d94-a159-05292a31ab3c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                8320      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**\n",
        "\n",
        "Compile and then fit the model "
      ],
      "metadata": {
        "id": "crjwuWaok_AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe1aF4w5kans",
        "outputId": "51a337dd-9fd3-486b-b9f4-b959357a1c5d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 36s 54ms/step - loss: 0.4133 - acc: 0.8187 - val_loss: 0.3134 - val_acc: 0.8676\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.2364 - acc: 0.9100 - val_loss: 0.2754 - val_acc: 0.8872\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.1851 - acc: 0.9315 - val_loss: 0.3421 - val_acc: 0.8854\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.1525 - acc: 0.9457 - val_loss: 0.2868 - val_acc: 0.8922\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.1323 - acc: 0.9546 - val_loss: 0.3970 - val_acc: 0.8440\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.1144 - acc: 0.9597 - val_loss: 0.3082 - val_acc: 0.8872\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.0975 - acc: 0.9681 - val_loss: 0.4849 - val_acc: 0.8724\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.0875 - acc: 0.9705 - val_loss: 0.3229 - val_acc: 0.8850\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.0787 - acc: 0.9741 - val_loss: 0.3564 - val_acc: 0.8818\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.0695 - acc: 0.9769 - val_loss: 0.3799 - val_acc: 0.8876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the model**\n",
        "\n",
        "Evaluate the model on the training data to see how well it performs. "
      ],
      "metadata": {
        "id": "qAUHSFRglRAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJloSFO1lZ75",
        "outputId": "c8b32609-6791-4c48-bba6-d7f5244129a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "483/782 [=================>............] - ETA: 4s - loss: 0.5060 - acc: 0.8433"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Predictions**\n",
        "\n",
        "Now we can use the network to make predictions on our own reviews. \n",
        "\n",
        "We have to convert our encoded reviews to a form that the network will understand. Load the encodings from the datasets and use them to encode our own data. "
      ],
      "metadata": {
        "id": "mgwTnmB5ljNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]  # added padding so it can be passed through the network\n",
        "\n",
        "text = 'that movie was just amazing, so amazing'\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "TYf3VGKynUFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make a decode function\n",
        "\n",
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = ''\n",
        "  for num in integers:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + ' '\n",
        "  return text[:-1]\n",
        "\n",
        "print(decode_integers(encoded))\n"
      ],
      "metadata": {
        "id": "alY3vGY6neEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction\n",
        "\n",
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250)) # make a blank numpy array with the shape 1 to 250\n",
        "  pred[0] = encoded_text # insert array of numbers to the first index \n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "  # if result >= 0.5: \n",
        "  #   ans = 'Positive review: ' + ' '.join((str(x) for x in result[0]))\n",
        "  # else:\n",
        "  #   ans = 'Negative review: ' + ' '.join((str(x) for x in result[0]))\n",
        "  # print(ans)\n",
        "\n",
        "positive_review = 'That movie was so good! I loved it and would watch it again because it was so great'\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = 'That movie sucked! I hated it and wouldn\\'t watch it again'\n",
        "predict(negative_review)\n"
      ],
      "metadata": {
        "id": "FHfIo-jQulnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dUFk-A9bxITr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}